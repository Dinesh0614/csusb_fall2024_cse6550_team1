{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Interactive Mode Demo\n",
    "This is a scripting page that prints actual raw data in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries\n",
    "1. Core Packages: dotenv, requests, httpx, and pymilvus.\n",
    "2. LangChain and Extensions:\n",
    "    * Langchain is used for processing language chains, and Milvus is the vector database for storing embeddings.\n",
    "    * langchain: Core package.\n",
    "    * langchain-core, langchain-mistralai, langchain-cohere, langchain-milvus, langchain-community, langchain-text-splitters, and langchain-huggingface for additional modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "Import the required libraries needed for the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Installing dependencies if not already installed\n",
    "!pip install os requests httpx pymilvus sqlite3\n",
    "!pip install langchain langchain-core langchain-mistralai langchain-cohere langchain-milvus langchain-community langchain-text-splitters langchain-huggingface\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymilvus import connections, utility\n",
    "import sqlite3\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display to confirm successful import\n",
    "print(\"Dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load env variables\n",
    "\n",
    "Load the env variables needed for operation of the RAG. `CORPUS_SOURCE` can be changed to load a different corpus. `MISTRAL_API_KEY` contains the MistralAi API key. `MILVUS_URI` is the path for the milvus lite database file. `MODEL_NAME` is the embedding model used on the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_SOURCE = 'https://www.csusb.edu/its'\n",
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "MILVUS_URI = \"milvus/jupyter_milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_store_check(uri):\n",
    "    \"\"\"\n",
    "    Returns response on whether the milvus database exists\n",
    "\n",
    "    Returns:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    \n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\",uri=uri)\n",
    "\n",
    "    # Return True if exists, False otherwise\n",
    "    return utility.has_collection(\"IT_support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    return embedding_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_web():\n",
    "    \"\"\"\n",
    "    Load the documents from the web and store the page contents\n",
    "\n",
    "    Returns:\n",
    "        list: The documents loaded from the web\n",
    "    \"\"\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=CORPUS_SOURCE,\n",
    "        prevent_outside=True,\n",
    "        base_url=CORPUS_SOURCE\n",
    "        )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Load an existing vector store\n",
    "    vector_store = Milvus(\n",
    "        collection_name=\"IT_support\",\n",
    "        embedding_function = get_embedding_function(),\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    print(\"Vector Store Loaded\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split the documents into chunks\n",
    "\n",
    "    Args:\n",
    "        documents (list): The documents to split\n",
    "\n",
    "    Returns:\n",
    "        list: list of chunks of documents\n",
    "    \"\"\"\n",
    "    # Create a text splitter to split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create a new vector store and drop any existing one\n",
    "    vector_store = Milvus.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"IT_support\",\n",
    "        connection_args={\"uri\": uri},\n",
    "        drop_old=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Vector Store Created\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a connection to the Milvus database with the specified collection name.\n",
    "# \"IT_support\" is the collection that stores embeddings related to IT support queries.\n",
    "# The `embedding_function` is a callable function that converts text to embeddings, \n",
    "# ensuring the retrieved data is in the same vector format as expected for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_milvus(uri: str=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Initialize the vector store for the RAG model\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    if vector_store_check(uri):\n",
    "        vector_store = load_existing_db(uri)\n",
    "        print(\"Embeddings loaded from existing Database\")\n",
    "    else:\n",
    "        embeddings = get_embedding_function()\n",
    "        print(\"Embeddings Loaded\")\n",
    "        documents = load_documents_from_web()\n",
    "        print(\"Documents Loaded\")\n",
    "        print(len(documents))\n",
    "    \n",
    "        # Split the documents into chunks\n",
    "        docs = split_documents(documents=documents)\n",
    "        print(\"Documents Splitting completed\")\n",
    "    \n",
    "        vector_store = create_vector_store(docs, embeddings, uri)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "initialize_milvus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for the prompt. This template provides the format and guidelines the AI assistant \n",
    "# should follow when generating responses to user questions. The assistant is instructed to use only \n",
    "# the information within the <context> tags, keeping answers fact-based and concise.\n",
    "# Create an instance of PromptTemplate using the defined template. This instance enables the AI assistant \n",
    "# to format responses according to the structure defined in PROMPT_TEMPLATE. The prompt accepts two input \n",
    "# variables, 'context' and 'question', which are dynamically filled in when the prompt is used in a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Created\n",
      "input_variables=['context', 'input'] template=\"\\n    Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\\n    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\\n    Only use the information provided in the <context> tags.\\n    If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n    <context>\\n    {context}\\n    </context>\\n\\n    <question>\\n    {input}\\n    </question>\\n\\n    The response should be specific and use statistics or numbers when possible.\\n\\n    Assistant:\"\n"
     ]
    }
   ],
   "source": [
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Create a prompt template for the RAG model\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The prompt template for the RAG model\n",
    "    \"\"\"\n",
    "    # Define the prompt template\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
    "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "    Only use the information provided in the <context> tags.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    <question>\n",
    "    {input}\n",
    "    </question>\n",
    "\n",
    "    The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    # Create a PromptTemplate instance with the defined template and input variables\n",
    "    prompt = PromptTemplate(\n",
    "        template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    print(\"Prompt Created\")\n",
    "\n",
    "    # Return the created prompt template to be used with the RAG model\n",
    "    return prompt\n",
    "\n",
    "print(create_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function sets up and runs a RAG model pipeline, integrating the following components:\n",
    "\n",
    "#     Model Initialization: Loads the ChatMistralAI model, which serves as the foundation for generating answers.\n",
    "#     Prompt Creation: Creates a prompt template for the model.\n",
    "#     Vector Store Loading: Loads the Milvus vector store to retrieve relevant document data.\n",
    "#     Retriever Setup: Configures a retriever to find and return relevant documents from the vector store.\n",
    "#     Document Chain and Retrieval Chain Creation: Sets up the chains that guide the RAG model through document processing and response generation.\n",
    "# Workflow\n",
    "#     Generate Response: The RAG model is invoked with the user query, generating an answer by leveraging the relevant documents.\n",
    "#     Error Handling: Handles any HTTP errors, including rate limits, by returning appropriate messages.\n",
    "#     Source Aggregation: Gathers and lists up to 4 unique sources relevant to the response, adding them as citations.\n",
    "#     Response Return: The function returns the generated response, enriched with cited sources.\n",
    "#     Arguments\n",
    "#         query (str): The input query string from the user.\n",
    "#     Returns\n",
    "#         str: The generated answer to the query, including sources if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = ChatMistralAI(model='open-mistral-7b')\n",
    "    print(\"Model Loaded\")\n",
    "\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_existing_db(uri=MILVUS_URI)\n",
    "    retriever = vector_store.as_retriever()\n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)\n",
    "        print(\"Document Chain Created\")\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        print(\"Retrieval Chain Created\")\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain.invoke({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:\n",
    "        print(f\"HTTPStatusError: {e}\")\n",
    "        if e.response.status_code == 429:\n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []\n",
    "        return \"I am unable to answer this question at the moment. Please try again later.\", []\n",
    "    \n",
    "    # logic to add sources to the response\n",
    "    max_relevant_sources = 4 # number of sources at most to be added to the response\n",
    "    all_sources = \"\"\n",
    "    sources = []\n",
    "    count = 1\n",
    "    for i in range(max_relevant_sources):\n",
    "        try:\n",
    "            source = response[\"context\"][i].metadata[\"source\"]\n",
    "            # check if the source is already added to the list\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "                all_sources += f\"[Source {count}]({source}), \"\n",
    "                count += 1\n",
    "        except IndexError: # if there are no more sources to add\n",
    "            break\n",
    "    all_sources = all_sources[:-2] # remove the last comma and space\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Response Generated:\")\n",
    "    \n",
    "    return response[\"answer\"]\n",
    "\n",
    "response = query_rag(\"how to connect wifi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
